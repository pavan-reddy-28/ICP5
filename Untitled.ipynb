{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c09559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.37209302325581395\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.19      0.44      0.27         9\n",
      "           2       0.33      0.16      0.21        19\n",
      "           3       0.33      0.20      0.25         5\n",
      "           5       0.00      0.00      0.00         2\n",
      "           6       0.67      1.00      0.80         2\n",
      "           7       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.37        43\n",
      "   macro avg       0.42      0.47      0.42        43\n",
      "weighted avg       0.40      0.37      0.36        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the glass data into a pandas dataframe\n",
    "glass_df = pd.read_csv(\"glass.csv\")\n",
    "\n",
    "# Divide the data into features and target variables\n",
    "features = glass_df.drop(\"Type\", axis=1)\n",
    "target = glass_df[\"Type\"]\n",
    "\n",
    "# Create training and testing datasets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=0)\n",
    "\n",
    "# Instatiate a Gaussian Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes model on the training data\n",
    "nb_model.fit(features_train, target_train)\n",
    "\n",
    "# Use the trained model to make predictions on the test data\n",
    "target_pred = nb_model.predict(features_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "print(\"Accuracy: \", accuracy_score(target_test, target_pred))\n",
    "print(\"Classification Report: \\n\", classification_report(target_test, target_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e12965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5116279069767442\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.36      0.89      0.52         9\n",
      "           2       0.58      0.37      0.45        19\n",
      "           3       0.00      0.00      0.00         5\n",
      "           5       0.50      0.50      0.50         2\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.86      1.00      0.92         6\n",
      "\n",
      "    accuracy                           0.51        43\n",
      "   macro avg       0.38      0.46      0.40        43\n",
      "weighted avg       0.48      0.51      0.46        43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pavanreddy/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/pavanreddy/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/pavanreddy/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the glass dataset\n",
    "glass_df = pd.read_csv(\"./glass.csv\")\n",
    "\n",
    "# Split the dataset into features and target variables\n",
    "features = glass_df.drop(\"Type\", axis=1)\n",
    "target = glass_df[\"Type\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create a linear SVM model\n",
    "svm_model = SVC(kernel=\"linear\")\n",
    "\n",
    "# Train the model on the training data\n",
    "svm_model.fit(features_train, target_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "target_pred = svm_model.predict(features_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print(\"Accuracy: \", accuracy_score(target_test, target_pred))\n",
    "print(\"Classification Report: \\n\", classification_report(target_test, target_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485eaf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive Bayes accuracy is 0.37209302325581395 so Naive Bayes got better accuracy.\n",
    "\n",
    "There could be several reasons why Naive Bayes might have achieved better accuracy compared to linear SVM:\n",
    "\n",
    "The dataset is simple and independent: \n",
    "    Naive Bayes assumes independence between features, so if the relationships between the features in the dataset are relatively simple, this algorithm might perform well.\n",
    "\n",
    "The dataset is large and high-dimensional: \n",
    "    Naive Bayes is computationally efficient and can handle large, high-dimensional datasets, so it might be a good choice for this type of data.\n",
    "\n",
    "The dataset has a lot of noisy or irrelevant features: \n",
    "    Naive Bayes can still produce good results even if the data has a lot of irrelevant features, as it doesn't put much weight on individual features.\n",
    "\n",
    "The classes in the dataset are well-separated: \n",
    "    Naive Bayes works well when the classes in the dataset are well-separated, so if this is the case, it might outperform other algorithms like linear SVM.\n",
    "    It's important to note that the choice of algorithm depends on the specific problem and dataset, so it's always a good idea to try multiple algorithms and evaluate their performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
